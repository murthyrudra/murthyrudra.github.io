<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>+++ title = “Intuition Behind Transfer Learning” date = 2019-05-05T00:00:00 subtitle = “” summary = “” authors = [] tags = [‘Transfer Learning’] categories = [‘Transfer Learning’] featured = false draft = false</p> <h1 id="list-format">List format.</h1> <h1 id="0--simple">0 = Simple</h1> <h1 id="1--detailed">1 = Detailed</h1> <h1 id="2--stream">2 = Stream</h1> <p>list_format = 2</p> <h1 id="optional-featured-image-relative-to-staticimg-folder">Optional featured image (relative to <code class="language-plaintext highlighter-rouge">static/img/</code> folder).</h1> <p>[header] image = “” caption = “” discussionId = 4 +++</p> <p>Some of us are really excited about the prospect of learning a new language. Unlike in regular language classes, we begin by learning the vocabulary. We ask for translations of real-world objects and day-to-day activities. We do not try to learn already known concepts, for example, the concept <em>tree</em>. Instead, we learn the corresponding word used to represent the concept (<em>tree</em>). This is different from how children first learn a language. We need to teach them concepts and also the symbols to represent those concepts in their language. This is what <strong>Transfer Learning</strong> is all about. <strong>Transfer Learning is using knowledge gained from one task to solve a related task.</strong></p> <h5 id="definition">Definition:</h5> <p>Now, let me quote the Wikipedia definition for <a href="https://en.wikipedia.org/wiki/Transfer_learning" rel="external nofollow noopener" target="_blank">Transfer Learning</a>,</p> <blockquote> <p>Transfer learning is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognise cars could apply when trying to recognise trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited.</p> </blockquote> <p>In the field of Natural Language Processing (NLP), transfer learning can take the form of multi-task learning or multilingual learning. Multi-task learning refers to using knowledge from one task to improve performance in a related task in the same language. For example, sentiment analysis helping emotion analysis. Similarly, multilingual learning refers to using knowledge from a task in one language to improve performance in a related task in another language language. Usually in multilingual learning, we focus on the case where the tasks are the same in both the languages.</p> <p>In this post, I won’t be going into the details of Transfer Learning. Sebastian Ruder’s blog post on Transfer Learning summarizes transfer learning. I will be motivating transfer learning from a slightly different perspective.</p> <h3 id="transfer-learning-a-strong-prior-distribution-over-models">Transfer Learning: A Strong Prior Distribution Over Models</h3> <p><em>Zoph et.al 2016</em> quotes in his paper titled <strong>Transfer Learning for Low-Resource Neural Machine Translation</strong>:</p> <blockquote> <p>A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialised with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training.</p> </blockquote> <h3 id="supervised-learning-as-curve-fitting">Supervised Learning as Curve Fitting</h3> <p>The goal of supervised learning can be thought of as fitting a curve (function) given some data points <em>i.e.,</em> $y = f(x)$. Learn the function $f$, given input values $x$ and corresponding output values $y$. If we had $y$ for every point in the input space, we can fit the curve exactly. This is not the case in real-life scenarios. We usually get to collect a subset of measurements $y$ for some sample of points $x$ in the input space. The measurements $y$ obtained might be noisy. As a consequence, there are many possible functions which can fit the data well. The problem reduces to <strong>search</strong>, where the goal is to find a function which fits the data.</p> <p>There are situations, where we rarely get to collect observations. The collected observations may not be indicative of the actual function to be learnt. Supervised learning fails badly in these cases. If we have a related task with abundant data points, we can train the supervised model on this auxiliary task. Now when we fine-tune the auxiliary task model on the actual task data, the model is already restricted in the function space. The search now proceeds to find a suitable function to fit the actual task data in the neighbourhood.</p> <h4 id="fitting-a-cosine-function">Fitting a Cosine Function</h4> <p>Let us demonstrate this intuition with a toy example. Consider the task of fitting a <strong>Cosine</strong> function. We will use a deep feed-forward neural network for this purpose. We will first generate a set of data points and plot the curve.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We will generate 200 random points from 0 to 720
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">720</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># Convert the values to radians
</span><span class="n">X_rad</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">180</span>
<span class="c1"># Get the corresponding cosine values for the inputs
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">X_rad</span><span class="p">)</span>
</code></pre></div></div> <p>The following figure plots the generated data points <img src="/assets/img/cosine.png" alt="cosine"> We now randomly selecting a subset of points (remember we used np.random to generate the points) to train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_small</span> <span class="o">=</span> <span class="n">X_rad</span><span class="p">[:</span><span class="mi">20</span><span class="p">]</span>
<span class="n">Y_small</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div></div> <p>The reduced set of data points leads to the following plot <img src="/assets/img/cosine-small.png" alt="cosine-small"> Before proceeding further, let us define the deep learning model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CurveFitter</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputDim</span><span class="p">,</span> <span class="n">outDim</span><span class="p">,</span> <span class="n">hidDim</span><span class="p">):</span>
		<span class="nf">super</span><span class="p">(</span><span class="n">CurveFitter</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

		<span class="n">self</span><span class="p">.</span><span class="n">inputDim</span> <span class="o">=</span> <span class="n">inputDim</span>
		<span class="n">self</span><span class="p">.</span><span class="n">outDim</span> <span class="o">=</span> <span class="n">outDim</span>
		<span class="n">self</span><span class="p">.</span><span class="n">hidDim</span> <span class="o">=</span> <span class="n">hidDim</span>

		<span class="n">self</span><span class="p">.</span><span class="n">linear_l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputDim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">)</span>
		<span class="n">self</span><span class="p">.</span><span class="n">nla_l1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>

		<span class="n">self</span><span class="p">.</span><span class="n">linear_l2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">)</span>
		<span class="n">self</span><span class="p">.</span><span class="n">nla_l2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>

		<span class="n">self</span><span class="p">.</span><span class="n">linear_l3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">)</span>
		<span class="n">self</span><span class="p">.</span><span class="n">nla_l3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>

		<span class="n">self</span><span class="p">.</span><span class="n">linear_l4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidDim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">outDim</span><span class="p">)</span>
		<span class="n">self</span><span class="p">.</span><span class="n">nla_l4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>

		<span class="n">self</span><span class="p">.</span><span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

		<span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l4</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l4</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l3</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l3</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l2</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l2</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l1</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span>

		<span class="k">return</span> <span class="n">result</span>

	<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>

		<span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l4</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l4</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l3</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l3</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l2</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l2</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">nla_l1</span><span class="p">(</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear_l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span> <span class="p">)</span>

		<span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>Let us instantiate the model and use <em>Adam</em> optimizer to train the model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">9899999</span><span class="p">)</span>
<span class="n">network_cos</span> <span class="o">=</span> <span class="nc">CurveFitter</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span> <span class="p">)</span>
<span class="n">optim_alg_cos</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">network_cos</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div> <p>Training the above model on the smaller set of points leads to the following curve <img src="/assets/img/cosine-supervised.png" alt="cosine-supervised"></p> <p>As observed, the model does well in areas where sufficient data is present. However, on the rightmost region where we have only one data point the model does poorly on fitting the curve.</p> <h4 id="transferring-knowledge-from-sine-function">Transferring Knowledge from Sine Function</h4> <p>Let us assume, that we have enough data points to train a Sine function. We will train the model using the earlier architecture. Unlike earlier, we will generate points from 0 to 1080 instead of 0 to 720.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We will generate 200 random points from 0 to 720
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1080</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># Convert the values to radians
</span><span class="n">X_rad</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">180</span>
<span class="c1"># Get the corresponding cosine values for the inputs
</span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X_rad</span><span class="p">)</span>
</code></pre></div></div> <p>The training points for sine function is <img src="/assets/img/sine.png" alt="sine"> and the curve fit by the model is <img src="/assets/img/sine-predicted.png" alt="sine-predicted"> We will use the trained model and further fine-tune the model on smaller set of points for the cosine function. The resulting fitted curve looks as below <img src="/assets/img/cosine-transfer.png" alt="cosine-transfer"></p> <p>Unlike the cosine model trained on smaller set of points, the fine-tuned cosine model does a reasonably decent job of mimicking the cosine function. The model also does a reasonable job outside of the domain. This is what I believe <em>Zoph et.al 2016</em> meant when they said that ** the parent model enforces a prior distribution on the function space, allowing the model to learn a function close-enough to the function to be fitted. Unlike random initializations where the possible function space is very large. **</p> <h4 id="initialization">Initialization</h4> <p>Also, the initialization of the deep learning model plays a crucial role. For different randomly initializations, model before training on the data fits the following curves <img src="/assets/img/cosine-rand.gif" alt="cosine-rand"> However, as seen earlier the transfer learning model has learnt the sine function. When we fine-tune the model searching for the cosine function becomes easier compared to randomly initialized models.</p> <p>The benefits might not be apparent in the toy example chosen. However, for very complex non-linear functions the benefits from transfer learning becomes clearer.</p> <h4 id="references">References</h4> <ul> <li>Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. <em>Transfer Learning for Low-Resource Neural Machine Translation</em>. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016.</li> </ul> </body></html>